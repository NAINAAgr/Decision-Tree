{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "A Decision Tree is a supervised learning model used for classification and regression tasks. In classification, the algorithm learns patterns from labeled data and represents decisions in the form of a tree-like structure with nodes and branches.\n",
        "\n",
        "Root Node â†’ contains the entire dataset and splits based on the most informative feature.\n",
        "\n",
        "Internal Nodes â†’ represent decisions based on feature values.\n",
        "\n",
        "Leaf Nodes â†’ represent the final class predictions.\n",
        "\n",
        "How it works (classification):\n",
        "\n",
        "Select the best feature to split data based on impurity measures (e.g., Gini, Entropy).\n",
        "\n",
        "Partition data into subsets.\n",
        "\n",
        "Repeat splitting until:\n",
        "\n",
        "All samples in a node belong to one class, or\n",
        "\n",
        "A stopping rule is reached (max depth, min samples).\n",
        "\n",
        "Example:\n",
        "A tree may first check petal length, then petal width to decide the flower species."
      ],
      "metadata": {
        "id": "U73gpi8FNj1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2\n",
        "Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact splits in a Decision Tree?\n",
        "\n",
        "Measure\t            Formula\t       Range\t              Interpretation\n",
        "\n",
        "Gini Impurity\t      1âˆ’âˆ‘(ð‘ð‘–2)\t   0to0.5 (binary)\t Lower Gini = purer node\n",
        "\n",
        "Entropy\t        âˆ’âˆ‘(ð‘ð‘–logâ¡2(ð‘ð‘–))   \t0 to 1\t   Measures disorder in data\n",
        "\n",
        "Impact on Tree Splitting:\n",
        "\n",
        "The feature that reduces impurity the most is selected for splitting.\n",
        "\n",
        "Lower post-split impurity â†’ better classification.\n",
        "\n",
        " Both metrics aim to create the purest child nodes, improving decision accuracy."
      ],
      "metadata": {
        "id": "DG2qnIbkNsR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3\n",
        "Difference between Pre-Pruning and Post-Pruning. Give one practical advantage each.\n",
        "\n",
        "Answer:\n",
        "Pre-Pruning\tPost-Pruning\n",
        "Stops tree growth early using constraints (e.g., max_depth, min_samples_split).\n",
        "\n",
        "Prevents complex trees from forming.\n",
        "\n",
        "Post-Pruning\n",
        "\n",
        "First builds full tree, then prunes branches that cause overfitting.\n",
        "\n",
        "Removes unnecessary branches after evaluation.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Pre-Pruning Advantage: Saves computation and reduces overfitting from the beginning.\n",
        "\n",
        "Post-Pruning Advantage: Usually gives better generalization because pruning decisions are based on actual model performance."
      ],
      "metadata": {
        "id": "RSjBxIgdPK8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4\n",
        "What is Information Gain in Decision Trees, and why is it important?\n",
        "\n",
        "Answer:\n",
        "Information Gain (IG) measures how much a feature improves class purity after splitting:\n",
        "\n",
        "ð¼ðº=ð¸ð‘›ð‘¡ð‘Ÿð‘œð‘ð‘¦(parent)âˆ’âˆ‘(ð‘›ð‘–/ð‘›)ð¸ð‘›ð‘¡ð‘Ÿð‘œð‘ð‘¦(childð‘–)\n",
        "\n",
        "\n",
        "High IG â†’ strong predictor â†’ best feature to split.\n",
        "\n",
        "Low IG â†’ weak separation of classes.\n",
        "\n",
        "Importance:\n",
        "It ensures the decision tree selects features that maximally reduce uncertainty, improving predictive accuracy."
      ],
      "metadata": {
        "id": "ctioZdd5Pn9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5\n",
        "Real-world applications, advantages, and limitations of Decision Trees\n",
        "\n",
        "Answer:\n",
        "Applications\n",
        "->Medical diagnosis (disease prediction)\n",
        "\n",
        "->Banking (loan approval, credit risk)\n",
        "\n",
        "->E-commerce (customer segmentation)\n",
        "\n",
        "->Fraud detection\n",
        "\n",
        "->Manufacturing quality control\n",
        "\n",
        "Advantages\n",
        "\n",
        "->Easy to interpret (graphical rules)\n",
        "\n",
        "->Handles numerical + categorical data\n",
        "\n",
        "->No need for feature scaling\n",
        "\n",
        "Limitations\n",
        "\n",
        "->Can overfit if grown fully\n",
        "\n",
        "->Slight changes in data may change structure (instability)\n",
        "\n",
        "->Less accurate than ensemble models (e.g., Random Forest)"
      ],
      "metadata": {
        "id": "5NuNzcpiQR-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6 : Python Program â€“ Iris dataset with Gini criterion\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFtWq0V5Q1tL",
        "outputId": "4e7655cc-ee15-4607-da76-df34eb0ccde3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7 : Compare Fully-grown Tree vs max_depth=3\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fully grown\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "# Limited depth\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "acc_limited = accuracy_score(y_test, limited_tree.predict(X_test))\n",
        "\n",
        "print(\"Full Tree Accuracy:\", acc_full)\n",
        "print(\"Max Depth=3 Accuracy:\", acc_limited)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qYd6XMLRNjR",
        "outputId": "2171c9d5-6679-4074-d5e3-2c3cb1fb325c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Max Depth=3 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FviCkCUjRfVt",
        "outputId": "57c368d9-1349-45c4-fa1d-d14bd3138009"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.5280096503174904\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9  :: Hyperparameter tuning using GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaR5lrGGR6F7",
        "outputId": "47a01b32-1ecb-4984-c9ad-b3a1fd8a4838"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10\n",
        "Healthcare Case Study â€“ Handling Missing Data & Training Decision Tree\n",
        "\n",
        "Answer:\n",
        "Step-by-step Process\n",
        "\n",
        "->Handle Missing Values\n",
        "\n",
        " . Numerical: mean/median imputation\n",
        "\n",
        " . Categorical: most frequent value\n",
        "\n",
        " . Advanced: model-based or KNN imputation\n",
        "\n",
        "->Encode Categorical Features\n",
        "\n",
        " . Label Encoding (for ordinal)\n",
        "\n",
        " . One-Hot Encoding (for nominal categories)\n",
        "\n",
        "->Split Dataset\n",
        "\n",
        "  . Trainâ€“Test split (e.g., 80â€“20)\n",
        "\n",
        "->Train Decision Tree\n",
        "\n",
        " . Use impurity measures to learn disease patterns\n",
        "\n",
        " . No scaling required\n",
        "\n",
        "->Hyperparameter Tuning\n",
        "\n",
        "  . max_depth, min_samples_split, criterion\n",
        "\n",
        "  . Use GridSearchCV for best model\n",
        "\n",
        "->Performance Evaluation\n",
        "\n",
        "  . Accuracy, Precision, Recall, F1-score\n",
        "\n",
        "  . Confusion matrix to detect misclassification of disease cases"
      ],
      "metadata": {
        "id": "B3xrfrtRSUTT"
      }
    }
  ]
}